{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "125314d2-3676-40dc-ab18-9bd877960631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn.init as init\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc2d356-26d5-429a-b5b4-9fbe81d32e53",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04065671-ef0e-4bc2-a7c5-43e3e777e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTScanDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for label, category in enumerate([\"Normal\", \"Covid\"]):\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            for img_name in os.listdir(category_path):\n",
    "                self.image_paths.append(os.path.join(category_path, img_name))\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = CTScanDataset(root_dir='/Users/rohanojha/Documents/01_Sem_1_DS 5220 Code/SML_Project/Code_Test/Train/', transform=transform)\n",
    "val_dataset   = CTScanDataset(root_dir='/Users/rohanojha/Documents/01_Sem_1_DS 5220 Code/SML_Project/Code_Test/Val/',   transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09470255-565d-40f5-a242-5ebea1a4a1d9",
   "metadata": {},
   "source": [
    "### Model 2 - 4 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68baae85-b1a0-4899-ab4a-af765d34bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Independent Channel Processing '''\n",
    "\n",
    "# '''\n",
    "# Changed the number of neurons in the hidden layers\n",
    "#         1. HL1 : 512\n",
    "#         2. HL2 : 256\n",
    "# Max Pool Kernel Size Changed to : 3x3\n",
    "# '''\n",
    "\n",
    "# class CustomCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomCNN, self).__init__()\n",
    "#         # Convolutional layers\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, groups=32)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, groups=64)\n",
    "#         self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, groups=128)\n",
    "        \n",
    "#         # Max pooling layer with 3x3 kernel\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "#         # Fully connected layers\n",
    "#         self.fc1 = nn.Linear(256 * 14 * 14, 512)  # Adjusted input size after pooling\n",
    "#         self.fc2 = nn.Linear(512, 256)        # Additional hidden layer\n",
    "#         self.fc3 = nn.Linear(256, 2)          # Binary classification layer\n",
    "\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Apply convolutional layers with ReLU activation and max pooling\n",
    "#         x = self.pool(self.relu(self.conv1(x)))\n",
    "#         x = self.pool(self.relu(self.conv2(x)))\n",
    "#         x = self.pool(self.relu(self.conv3(x)))\n",
    "#         x = self.pool(self.relu(self.conv4(x)))\n",
    "        \n",
    "#         # Flatten the tensor to feed into fully connected layers\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        \n",
    "#         # Apply fully connected layers with dropout and ReLU activation\n",
    "#         x = self.dropout(self.relu(self.fc1(x)))\n",
    "#         x = self.dropout(self.relu(self.fc2(x)))  # Additional hidden layer\n",
    "#         x = self.fc3(x)  # Output layer\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f186e756-3918-42b1-b2e6-347e3271fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Independent Channel Processing '''\n",
    "\n",
    "# '''\n",
    "# Changed the number of neurons in the hidden layers\n",
    "#         1. HL1 : 512\n",
    "#         2. HL2 : 256\n",
    "# Max Pool Kernel Size Changed to : 3x3\n",
    "# Using HE Initialization for weights and biases:\n",
    "#         1. Prevents gradient decay in ReLU\n",
    "#         2. Works better for deeper networks(normally greater >3 layers)\n",
    "#         3. Works for ReLU, Leaky ReLU\n",
    "\n",
    "# Xavier Initialization for weights and biases:\n",
    "#         1. Balanced gradients\n",
    "#         2. Works well for shallow networks(normally greater <=3 layers)\n",
    "#         3. Works for Sigmoid, Tanh\n",
    "# '''\n",
    "\n",
    "# class CustomCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomCNN, self).__init__()\n",
    "#         # Convolutional layers\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, groups=32)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, groups=64)\n",
    "#         self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, groups=128)\n",
    "        \n",
    "#         # Max pooling layer with 3x3 kernel\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "#         # Fully connected layers\n",
    "#         self.fc1 = nn.Linear(256 * 14 * 14, 512)  # Adjusted input size after pooling\n",
    "#         self.fc2 = nn.Linear(512, 256)        # Additional hidden layer\n",
    "#         self.fc3 = nn.Linear(256, 2)          # Binary classification layer\n",
    "\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "#         # Apply He Initialization to all layers\n",
    "#         self._initialize_weights()\n",
    "\n",
    "#     def _initialize_weights(self):\n",
    "#         # Loop through all layers in the module\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):  # For convolutional layers\n",
    "#                 init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     init.zeros_(m.bias)\n",
    "#             elif isinstance(m, nn.Linear):  # For fully connected layers\n",
    "#                 init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     init.zeros_(m.bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Apply convolutional layers with ReLU activation and max pooling\n",
    "#         x = self.pool(self.relu(self.conv1(x)))\n",
    "#         x = self.pool(self.relu(self.conv2(x)))\n",
    "#         x = self.pool(self.relu(self.conv3(x)))\n",
    "#         x = self.pool(self.relu(self.conv4(x)))\n",
    "        \n",
    "#         # Flatten the tensor to feed into fully connected layers\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        \n",
    "#         # Apply fully connected layers with dropout and ReLU activation\n",
    "#         x = self.dropout(self.relu(self.fc1(x)))\n",
    "#         x = self.dropout(self.relu(self.fc2(x)))  # Additional hidden layer\n",
    "#         x = self.fc3(x)  # Output layer\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f9e5e46-a4a4-4d13-b393-e6558a26588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Independent Channel Processing '''\n",
    "\n",
    "'''\n",
    "- Changed the number of neurons in the hidden layers\n",
    "        1. HL1 : 512\n",
    "        2. HL2 : 256\n",
    "- Max Pool Kernel Size Changed to : 3x3\n",
    "- Activation Function: Leaky ReLU\n",
    "- Using HE Initialization for weights and biases:\n",
    "        1. Prevents gradient decay in ReLU\n",
    "        2. Works better for deeper networks(normally greater >3 layers)\n",
    "        3. Works for ReLU, Leaky ReLU\n",
    "\n",
    "- Xavier Initialization for weights and biases:\n",
    "        1. Balanced gradients\n",
    "        2. Works well for shallow networks(normally greater <=3 layers)\n",
    "        3. Works for Sigmoid, Tanh\n",
    "'''\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, groups=32)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, groups=64)\n",
    "        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, groups=256)\n",
    "        \n",
    "        # Max pooling layer with 3x3 kernel\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)  # Adjusted input size after pooling\n",
    "        self.fc2 = nn.Linear(512, 256)        # Additional hidden layer\n",
    "        self.fc3 = nn.Linear(256, 2)          # Binary classification layer\n",
    "\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Activation and dropout\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.01) \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # Apply He Initialization to all layers\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Loop through all layers in the module\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):  # For convolutional layers\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):  # For fully connected layers\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with ReLU activation and max pooling\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = self.pool(self.activation(self.conv3(x)))\n",
    "        x = self.pool(self.activation(self.conv4(x)))\n",
    "        \n",
    "        # Flatten the tensor to feed into fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply fully connected layers with dropout and ReLU activation\n",
    "        x = self.dropout(self.activation(self.fc1(x)))\n",
    "        x = self.dropout(self.activation(self.fc2(x)))  # Additional hidden layer\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944c56a-69f2-4304-9155-ad841c814432",
   "metadata": {},
   "source": [
    "### Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91a1babc-0d92-43f6-ba1a-55769f097820",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CustomCNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b80b0b-53e7-4299-a5aa-88326eb6defe",
   "metadata": {},
   "source": [
    "### Train -- Freezing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd45edc3-2059-4aac-85c1-9a62aeab521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.9851, Accuracy: 49.35%\n",
      "Epoch [2/10], Loss: 0.7448, Accuracy: 49.46%\n",
      "Epoch [3/10], Loss: 0.6802, Accuracy: 59.24%\n",
      "Epoch [4/10], Loss: 0.5474, Accuracy: 73.91%\n",
      "Epoch [5/10], Loss: 0.4238, Accuracy: 80.76%\n",
      "Epoch [6/10], Loss: 0.2750, Accuracy: 89.57%\n",
      "Epoch [7/10], Loss: 0.1569, Accuracy: 94.57%\n",
      "Epoch [8/10], Loss: 0.1046, Accuracy: 96.96%\n",
      "Epoch [9/10], Loss: 0.1252, Accuracy: 95.76%\n",
      "Epoch [10/10], Loss: 0.1025, Accuracy: 97.28%\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f3dd4a5-cb6f-4ca4-a5fc-67a4ec717155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7696, Accuracy: 52.28%\n",
      "Epoch [2/10], Loss: 0.6849, Accuracy: 55.54%\n",
      "Epoch [3/10], Loss: 0.6315, Accuracy: 62.93%\n",
      "Epoch [4/10], Loss: 0.5358, Accuracy: 72.17%\n",
      "Epoch [5/10], Loss: 0.4660, Accuracy: 78.80%\n",
      "Epoch [6/10], Loss: 0.4393, Accuracy: 79.67%\n",
      "Epoch [7/10], Loss: 0.3840, Accuracy: 83.04%\n",
      "Epoch [8/10], Loss: 0.3739, Accuracy: 84.46%\n",
      "Epoch [9/10], Loss: 0.3195, Accuracy: 86.85%\n",
      "Epoch [10/10], Loss: 0.2771, Accuracy: 89.13%\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d685d99-97db-4592-8267-a304dae02464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6810, Accuracy: 56.20%\n",
      "Epoch [2/10], Loss: 0.6433, Accuracy: 62.83%\n",
      "Epoch [3/10], Loss: 0.5091, Accuracy: 79.02%\n",
      "Epoch [4/10], Loss: 0.4142, Accuracy: 82.61%\n",
      "Epoch [5/10], Loss: 0.2895, Accuracy: 89.78%\n",
      "Epoch [6/10], Loss: 0.2239, Accuracy: 92.28%\n",
      "Epoch [7/10], Loss: 0.1458, Accuracy: 96.20%\n",
      "Epoch [8/10], Loss: 0.1142, Accuracy: 96.85%\n",
      "Epoch [9/10], Loss: 0.0767, Accuracy: 98.91%\n",
      "Epoch [10/10], Loss: 0.0629, Accuracy: 99.02%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5a21b-a377-4a05-933e-812fea678c48",
   "metadata": {},
   "source": [
    "### Model Evaluation -- Freezing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77d5587b-7cab-4d8a-9043-08fe44d80c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 70.69%\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for images, labels in val_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59a35b90-c330-4d35-bce6-9dbd94a40265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 75.43%\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for images, labels in val_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b701fca8-b009-4ec6-8a05-74d610b47880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 63.79%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792379a-4711-4dc2-a768-ed09f2c8784b",
   "metadata": {},
   "source": [
    "### Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a9e462b0-83d1-458b-a223-71dd283f73f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Normal\n"
     ]
    }
   ],
   "source": [
    "# Save the Model\n",
    "torch.save(model.state_dict(), \"covid_ct_model_ICP_K32_256_HL_1_2_512Neurons_3MxPl_Drpout_30.pth\")\n",
    "\n",
    "# Load the model\n",
    "model = CustomCNN()\n",
    "model.load_state_dict(torch.load(\"covid_ct_model_ICP_K32_256_HL_1_2_512Neurons_3MxPl_Drpout_30.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Predict on a single image\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transform used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "image_path = \"/Users/rohanojha/Documents/01_Sem_1_DS 5220 Code/SML_Project/Code_Test/single_prediction/normal.png\"  # Replace with the actual path\n",
    "# image_path = \"/Users/rohanojha/Documents/01_Sem_1_DS 5220 Code/SML_Project/Code_Test/single_prediction/covid.png\"\n",
    "# image_path = \"/Users/rohanojha/Documents/01_Sem_1_DS 5220 Code/SML_Project/Code_Test/single_prediction/Non-Covid (10).png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Apply transformations\n",
    "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move the tensor to the same device as the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_tensor = image_tensor.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    output = model(image_tensor)\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "# Map predicted class to label\n",
    "classes = [\"Normal\", \"Covid\"]\n",
    "prediction = classes[predicted_class.item()]\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
