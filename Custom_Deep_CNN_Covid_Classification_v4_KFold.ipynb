{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "125314d2-3676-40dc-ab18-9bd877960631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn.init as init\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc2d356-26d5-429a-b5b4-9fbe81d32e53",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04065671-ef0e-4bc2-a7c5-43e3e777e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTScanDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for label, category in enumerate([\"Normal\", \"Covid\"]):\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            for img_name in os.listdir(category_path):\n",
    "                self.image_paths.append(os.path.join(category_path, img_name))\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = CTScanDataset(root_dir='/Users/rohanojha/Documents/01_Sem_1_DS 5220 Code/SML_Project/Code_Test/Train/', transform=transform)\n",
    "test_dataset   = CTScanDataset(root_dir='/Users/rohanojha/Documents/01_Sem_1_DS 5220 Code/SML_Project/Code_Test/Val/',   transform=transform)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09470255-565d-40f5-a242-5ebea1a4a1d9",
   "metadata": {},
   "source": [
    "### Model 2 - 4 Layer CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9410724-f7e8-4c8b-ac58-93f28a1f2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Independent Channel Processing '''\n",
    "\n",
    "'''\n",
    "Changed the number of neurons in the hidden layers\n",
    "        1. HL1 : 512\n",
    "        2. HL2 : 256\n",
    "Max Pool Kernel Size Changed to : 3x3\n",
    "Using HE Initialization for weights and biases:\n",
    "        1. Prevents gradient decay in ReLU\n",
    "        2. Works better for deeper networks(normally greater >3 layers)\n",
    "        3. Works for ReLU, Leaky ReLU\n",
    "\n",
    "Xavier Initialization for weights and biases:\n",
    "        1. Balanced gradients\n",
    "        2. Works well for shallow networks(normally greater <=3 layers)\n",
    "        3. Works for Sigmoid, Tanh\n",
    "'''\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, groups=32)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, groups=64)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, groups=128)\n",
    "        \n",
    "        # Max pooling layer with 3x3 kernel\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)  # Adjusted input size after pooling\n",
    "        self.fc2 = nn.Linear(512, 256)        # Additional hidden layer\n",
    "        self.fc3 = nn.Linear(256, 2)          # Binary classification layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # Apply He Initialization to all layers\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Loop through all layers in the module\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):  # For convolutional layers\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):  # For fully connected layers\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with ReLU activation and max pooling\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.pool(self.relu(self.conv4(x)))\n",
    "        \n",
    "        # Flatten the tensor to feed into fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply fully connected layers with dropout and ReLU activation\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))  # Additional hidden layer\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Loss and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b80b0b-53e7-4299-a5aa-88326eb6defe",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785aac1-7777-4930-88b5-010b818361b3",
   "metadata": {},
   "source": [
    "#### Freezing This"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e956e52b-ee24-4d5c-8bb7-3edd17b48047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 0.738676579102226\n",
      "Epoch 2, Loss: 0.6100599804650182\n",
      "Epoch 3, Loss: 0.4069567374561144\n",
      "Epoch 4, Loss: 0.2181377453000649\n",
      "Epoch 5, Loss: 0.14224674325922262\n",
      "Epoch 6, Loss: 0.10155483003219833\n",
      "Epoch 7, Loss: 0.07827273760314869\n",
      "Epoch 8, Loss: 0.048444662121650967\n",
      "Epoch 9, Loss: 0.061293622428227376\n",
      "Epoch 10, Loss: 0.0553135316737968\n",
      "Fold 1, Validation Accuracy: 95.11%\n",
      "Fold 2\n",
      "Epoch 1, Loss: 0.7375401154808376\n",
      "Epoch 2, Loss: 0.6950695359188578\n",
      "Epoch 3, Loss: 0.608172937579777\n",
      "Epoch 4, Loss: 0.4746983051300049\n",
      "Epoch 5, Loss: 0.3281523786161257\n",
      "Epoch 6, Loss: 0.20465769003266873\n",
      "Epoch 7, Loss: 0.18650074789057608\n",
      "Epoch 8, Loss: 0.11212774414731108\n",
      "Epoch 9, Loss: 0.099261822907821\n",
      "Epoch 10, Loss: 0.07372033814697163\n",
      "Fold 2, Validation Accuracy: 97.83%\n",
      "Fold 3\n",
      "Epoch 1, Loss: 0.7531616895095162\n",
      "Epoch 2, Loss: 0.6818736921186033\n",
      "Epoch 3, Loss: 0.5864553555198337\n",
      "Epoch 4, Loss: 0.523631126984306\n",
      "Epoch 5, Loss: 0.4691731592883234\n",
      "Epoch 6, Loss: 0.42762855472772016\n",
      "Epoch 7, Loss: 0.39287893085376074\n",
      "Epoch 8, Loss: 0.35451444853907044\n",
      "Epoch 9, Loss: 0.3175629435674004\n",
      "Epoch 10, Loss: 0.2467873355616694\n",
      "Fold 3, Validation Accuracy: 85.33%\n",
      "Fold 4\n",
      "Epoch 1, Loss: 0.7823321404664413\n",
      "Epoch 2, Loss: 0.6478729688602946\n",
      "Epoch 3, Loss: 0.4307946115732193\n",
      "Epoch 4, Loss: 0.3083110069451125\n",
      "Epoch 5, Loss: 0.2268576191171356\n",
      "Epoch 6, Loss: 0.1391581212696822\n",
      "Epoch 7, Loss: 0.12422365857207257\n",
      "Epoch 8, Loss: 0.09560912406153005\n",
      "Epoch 9, Loss: 0.04623040933485912\n",
      "Epoch 10, Loss: 0.03569223361251795\n",
      "Fold 4, Validation Accuracy: 98.37%\n",
      "Fold 5\n",
      "Epoch 1, Loss: 0.7994293855584186\n",
      "Epoch 2, Loss: 0.6942497621411863\n",
      "Epoch 3, Loss: 0.6455207518909288\n",
      "Epoch 4, Loss: 0.5294202695722166\n",
      "Epoch 5, Loss: 0.4273213886696359\n",
      "Epoch 6, Loss: 0.36939496320226917\n",
      "Epoch 7, Loss: 0.24515622271143872\n",
      "Epoch 8, Loss: 0.24566901572372601\n",
      "Epoch 9, Loss: 0.20158501390529715\n",
      "Epoch 10, Loss: 0.1455265635057636\n",
      "Fold 5, Validation Accuracy: 97.28%\n"
     ]
    }
   ],
   "source": [
    "# k = 5\n",
    "# kfold = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# # Initialize results dictionary\n",
    "# results = {}\n",
    "\n",
    "# # Loop through k-folds\n",
    "# for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "#     print(f'Fold {fold + 1}')\n",
    "    \n",
    "#     # Subset datasets for the current fold\n",
    "#     train_subsampler = Subset(train_dataset, train_ids)\n",
    "#     val_subsampler = Subset(train_dataset, val_ids)\n",
    "    \n",
    "#     # Create data loaders    \n",
    "#     train_loader = DataLoader(train_subsampler, batch_size=32, shuffle=True)\n",
    "#     val_loader = DataLoader(val_subsampler, batch_size=32, shuffle=False)\n",
    "    \n",
    "#     # Initialize the model\n",
    "#     model = CustomCNN().to(device)\n",
    "    \n",
    "#     # Define loss function and optimizer\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "#     # Training loop\n",
    "#     for epoch in range(10):  # Adjust number of epochs as needed\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for inputs, labels in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "        \n",
    "#         print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
    "    \n",
    "#     # Validation loop\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     # Save accuracy for the current fold\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f'Fold {fold + 1}, Validation Accuracy: {accuracy:.2f}%')\n",
    "#     results[fold] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5a21b-a377-4a05-933e-812fea678c48",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd157e-68e5-4a10-89b3-eaaa5c038286",
   "metadata": {},
   "source": [
    "#### Freezing This"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d5d371b-7401-4cc8-9fcb-9a34e06cf6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for images, labels in test_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792379a-4711-4dc2-a768-ed09f2c8784b",
   "metadata": {},
   "source": [
    "### Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9e462b0-83d1-458b-a223-71dd283f73f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Normal\n"
     ]
    }
   ],
   "source": [
    "# Save the Model\n",
    "# torch.save(model.state_dict(), \"covid_ct_model_ICP_K32_256_HL_1_2_512Neurons_3MxPl_Drpout_30_KFold.pth\")\n",
    "\n",
    "# Load the model\n",
    "model = CustomCNN()\n",
    "# model.load_state_dict(torch.load(\"covid_ct_model_ICP_K32_256_HL_1_2_512Neurons_3MxPl_Drpout_30_KFold.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Predict on a single image\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transform used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "image_path = \"/Users/rohanojha/Documents/01_Sem_1_DS 5220 Code/SML_Project/Code_Test/single_prediction/normal.png\"  # Replace with the actual path\n",
    "# image_path = \"/Users/rohanojha/Documents/01_Sem_1_DS 5220 Code/SML_Project/Code_Test/single_prediction/covid.png\"\n",
    "# image_path = \"/Users/rohanojha/Documents/01_Sem_1_DS 5220 Code/SML_Project/Code_Test/single_prediction/Non-Covid (10).png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Apply transformations\n",
    "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move the tensor to the same device as the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_tensor = image_tensor.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    output = model(image_tensor)\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "# Map predicted class to label\n",
    "classes = [\"Normal\", \"Covid\"]\n",
    "prediction = classes[predicted_class.item()]\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
